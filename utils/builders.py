import torch
import torch.nn as nn
from optimizers import SCALE


def build_model(model, args):
    return model


def build_optimizer(model, trainable_params, args):
    if args.optimizer.lower() == "adam":
        optimizer = torch.optim.Adam(
            trainable_params, lr=args.lr, weight_decay=args.weight_decay
        )

    elif args.optimizer.lower() == "scale":
        main_params = []
        oned_params = []
        secondary_params = []
        main_modules_list = ["attn", "mlp", "attention", "embed_tokens"]

        print(f"MAIN MODULES = {main_modules_list} !!!")

        id_to_name_main_params = {}
        id_to_name_secondary_params = {}
        id_to_name_oned_params = {}

        for module_name, module in model.named_modules():
            if not (isinstance(module, nn.Linear) or isinstance(module, nn.Embedding)):
                continue
            if not any(target_key in module_name for target_key in main_modules_list):
                continue

            main_params.append(module.weight)
            id_to_name_main_params[id(module.weight)] = module_name

        for param_name, p in model.named_parameters():
            if id(p) in id_to_name_main_params:
                continue

            if p.ndim == 1:
                oned_params.append(p)
                id_to_name_oned_params[id(p)] = param_name
            else:
                secondary_params.append(p)
                id_to_name_secondary_params[id(p)] = param_name

        for module_name, module in model.named_modules():
            if hasattr(module, 'weight'):
                p = module.weight
                if id(p) in id_to_name_main_params:
                    print("Main module: ", module_name)
                if id(p) in id_to_name_oned_params:
                    print("1D module: ", module_name)
                if id(p) in id_to_name_secondary_params:
                    print("Secondary module: ", module_name)

        id_to_name = {**id_to_name_main_params, **id_to_name_secondary_params, **id_to_name_oned_params}

        # Create the optimizer
        optimizer = SCALE(
            lr=args.lr,
            wd=args.weight_decay,
            main_params=main_params,
            secondary_params=secondary_params,
            oned_params=oned_params,
            id_to_name=id_to_name,
            debug=args.debug,
            momentum=args.momentum,
            adam_lr=args.adam_lr,
            adamw_betas=(args.adam_beta_1, args.adam_beta_2),
            adamw_eps=1e-8,
        )

    elif args.optimizer.lower() == "adamw":
        optimizer = torch.optim.AdamW(
            trainable_params, lr=args.lr, weight_decay=args.weight_decay
        )

    elif args.optimizer.lower() == "sgd":
        optimizer = torch.optim.SGD(
            trainable_params,
            lr=args.lr,
            weight_decay=args.weight_decay,
            momentum=args.beta1,
        )
    else:
        raise ValueError(f"Optimizer {args.optimizer} not supported")

    return optimizer
